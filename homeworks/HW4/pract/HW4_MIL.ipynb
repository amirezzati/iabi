{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW4 - Breast Cancer Detection using Multiple Instance Learning (MIL) (100 points)\n",
        "\n",
        "## Course Name: Intelligent Analysis of Biomedical Images\n",
        "\n",
        "#### Lecturers: Dr. Rohban\n",
        "#### Name: \n",
        "#### Student ID: \n",
        "\n",
        "---\n",
        "\n",
        "**Contact**: Ask your questions in Quera\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbrB0MgxhWqi"
      },
      "source": [
        "In this part we are going implement the idea proposed by the paper \"Breast Cancer Histopathology Image Classification\n",
        "and Localization using Multiple Instance Learning\" and reproduce some of their results. Please read this paper before starting the next part. Here is the link to it:\n",
        "\n",
        "https://arxiv.org/pdf/2003.00823.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq_v4elarDmA"
      },
      "source": [
        "In the next sections, we only use one of the presented datasets in the above paper. In the explanation of every section, it is assumed that you have read the paper before starting that part. It should also be mentioned that you are free to modify everything in this notebook and all provided codes are for better understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w6BPc4JQdc3"
      },
      "source": [
        "You can read the \"Attention-based Deep Multiple Instance Learning\" paper for more explanation. Here is the link to it:\n",
        "\n",
        "https://arxiv.org/pdf/1802.04712.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt_IZkamipl0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtR6EJb-jqkf"
      },
      "source": [
        "Feel free to import any library you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MGEqdP8ZsDUN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJKoh50j3XF"
      },
      "source": [
        "## Downloading dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWW9K4rAj_5w"
      },
      "source": [
        "Here is the code to download and decompress the BreaKHis dataset. This code is written for the google colab environment and you may need to modify it for running on other devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ6fFnhfmOTQ"
      },
      "source": [
        "Link to the dataset's website:\n",
        "\n",
        "https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjL8Nyjbf8p9",
        "outputId": "a49ea5e7-76d9-48f5-e0c8-ec00f7059816"
      },
      "outputs": [],
      "source": [
        "!wget http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz -O /content/BreaKHis_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8xDYIqkigjyU"
      },
      "outputs": [],
      "source": [
        "!tar xzf /content/BreaKHis_v1.tar.gz -C /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSYOh2nlKx6"
      },
      "source": [
        "## Data Preparation (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn7TF-kIprVl"
      },
      "source": [
        "In this section, you will implement several things:\n",
        "\n",
        "- A custom pytorch Dataset for our data. You can use the current decompressed folder or change the structure to make it easier to work with.\n",
        "- Transformations to perform on data (You must first do the patch extraction, then apply the transformations on every patch.)\n",
        "- Patch extraction code (you can also implement it in the following sections)\n",
        "- Dataloaders (instead of 80-20 split for train-test sets from the paper, you should use 80-5-15 split for train-val-test sets here and save best model w.r.t. the validation loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bc-UHR4TiPxd"
      },
      "outputs": [],
      "source": [
        "# Implement the custom dataset here (15 points)\n",
        "class CustomBreaKHis(Dataset):\n",
        "    def __init__(self, path_to_data, magn, transform=None):\n",
        "        # To-Do\n",
        "\n",
        "    def __len__(self):\n",
        "        # To-Do\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # To-Do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AVM3hBd4LSsD"
      },
      "outputs": [],
      "source": [
        "# Implement the transforms here (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7uqiQzQ8RB4w"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders here (5 points)\n",
        "magnifications = [\"40X\", \"100X\", \"200X\", \"400X\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "okFoJzT97fAl"
      },
      "outputs": [],
      "source": [
        "# Implement patch extraction here (10 points)\n",
        "# If you are going to implement it somewhere else, please comment it here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RX02sSeLblU"
      },
      "source": [
        "## Model Implementation (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a34eqHzLkB2"
      },
      "source": [
        "Here, you have to implement the proposed architecture in the paper. The only point is that in the last part of the model, instead of a \"dense+softmax\" you should implement a \"dense+sigmoid\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MGC-Hwdfygoz"
      },
      "outputs": [],
      "source": [
        "# Implement the model here (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGvECwd-NGJ4"
      },
      "source": [
        "## Training (32 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h03lnx3NJoF"
      },
      "source": [
        "In this section, you will implement the training method and use it to train the model for every magnification. The model has to be trained for at least 60 epochs. But, in every epoch, we use 200 mini-batches to train the model instead of the whole training data, which means with a batch size of one, we use 200 images for training the model in every epoch. With this method, we spend less training time but actually, iterate whole data several times. (You have to make sure that the train loader will get shuffled correctly every epoch)\n",
        "\n",
        "You should save loss and accuracy per epoch for training and validation sets. Then, you must plot the loss and accuracy of every magnification, which is crucial for scoring this assignment.\n",
        "\n",
        "Hint: Training the model for one epoch takes around 7 seconds on the Google Colab environment. You have to train every model for at least 60 epochs, but longer training time can result in higher accuracies.\n",
        "\n",
        "Note: You are not expected to achieve the exact accuracies reported in the paper for getting the full score. A correct implementation and sufficient training epochs receive the total score for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jA2rmBJwzQeG"
      },
      "outputs": [],
      "source": [
        "# Implement the training method here (20 points)\n",
        "max_steps = 200\n",
        "def train():\n",
        "    # To-Do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfaes8urbz5f",
        "outputId": "73008485-7204-4dfa-e17a-1209821c0bdd"
      },
      "outputs": [],
      "source": [
        "# Train the model on the 40X data here (1 points)\n",
        "epochs = 60\n",
        "magn = \"40X\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dl2gxL7TOyAA",
        "outputId": "aa39b949-d9b8-4e61-e00c-fe3f98247787"
      },
      "outputs": [],
      "source": [
        "# Plot loss and accuracy per epoch for 40X here (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVXLM3NMJjDT",
        "outputId": "fde3406e-f650-4a85-8216-9a2440a37a08"
      },
      "outputs": [],
      "source": [
        "# Train the model on the 100X data here (1 points)\n",
        "epochs = 60\n",
        "magn = \"100X\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kbab1mVOPtow",
        "outputId": "134a7574-50af-4a04-f71f-8f2c240729f4"
      },
      "outputs": [],
      "source": [
        "# Plot loss and accuracy per epoch for 100X here (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx0ZKmCNJnRl",
        "outputId": "24dc0930-258a-47b7-bb63-03ef4775b957"
      },
      "outputs": [],
      "source": [
        "# Train the model on the 200X data here (1 points)\n",
        "epochs = 60\n",
        "magn = \"200X\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "etpskv0pUNJ7",
        "outputId": "46d95c01-ff1d-4caf-99c2-b1ddb44180aa"
      },
      "outputs": [],
      "source": [
        "# Plot loss and accuracy per epoch for 200X here (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfuzI_3TJvIo",
        "outputId": "015769ee-eb75-441b-854e-e32818d32a02"
      },
      "outputs": [],
      "source": [
        "# Train the model on the 400X data here (1 points)\n",
        "epochs = 60\n",
        "magn = \"400X\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0v9zUwHJURBd",
        "outputId": "9ae0342d-eec9-43f3-bf4e-310f2a0de162"
      },
      "outputs": [],
      "source": [
        "# Plot loss and accuracy per epoch for 400X here (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JwCsjXBVvV9"
      },
      "source": [
        "## Evaluation (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtoCmy9gZeQP"
      },
      "source": [
        "Now, we need to evaluate our best model in every magnification on the test set. Complete the test method and use it for the evaluation of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "orbk7Pa2Vxxj"
      },
      "outputs": [],
      "source": [
        "# Implement the test method here (4 points)\n",
        "def test():\n",
        "    # To-Do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6wb5hj1Sr_W",
        "outputId": "22cbeb65-cb8c-4039-8d21-a430c91879f4"
      },
      "outputs": [],
      "source": [
        "# Test the model trained on the 40X data here (1 points)\n",
        "magn = \"40X\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bwqS1isnXGY",
        "outputId": "0f7fdbb6-b576-4388-fc30-7f86bffee786"
      },
      "outputs": [],
      "source": [
        "# Test the model trained on the 100X data here (1 points)\n",
        "magn = \"100X\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8m0OmxCnrT7",
        "outputId": "248dc281-7cb7-43d4-97c6-b12a8517c62f"
      },
      "outputs": [],
      "source": [
        "# Test the model trained on the 200X data here (1 points)\n",
        "magn = \"200X\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo3CxsBbSvYV",
        "outputId": "4ec6affc-0fd0-4529-acef-53263fd67e00"
      },
      "outputs": [],
      "source": [
        "# Test the model trained on the 400X data here (1 points)\n",
        "magn = \"400X\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
