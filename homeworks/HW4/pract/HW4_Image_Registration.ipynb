{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW4 - Image Registration\n",
        "\n",
        "## Course Name: Intelligent Analysis of Biomedical Images\n",
        "\n",
        "#### Lecturers: Dr. Rohban\n",
        "#### Name: \n",
        "#### Student ID: \n",
        "\n",
        "---\n",
        "\n",
        "**Contact**: Ask your questions in Quera\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3POtrjRyom"
      },
      "source": [
        "# Image Registration using VoxelMorph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omg61bVMZs5y"
      },
      "source": [
        "**Introduction**:\n",
        "\n",
        "Image registration is a critical process in medical image analysis, aiming to align two or more images for comparative and diagnostic purposes. This task becomes increasingly vital in scenarios involving different imaging modalities or temporal intervals.\n",
        "\n",
        "VoxelMorph, an established approach in this field, utilizes deep learning and the U-Net architecture for streamlined image registration. It accurately aligns images by learning deformations, making it advantageous for complex spatial relationships and detailed feature preservation in medical image analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvxkZzv2RH37"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7_ZyQy0YoqJ"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBP_Kyoq1UMz"
      },
      "source": [
        "**Cross Correlation Loss:**\n",
        "- Calculates the cross-correlation loss between two input tensors.\n",
        "- Encourages the model to produce similar outputs to the ground truth.\n",
        "\n",
        "**Smoothing Loss:**\n",
        "- Measures smoothness by comparing neighboring pixel differences.\n",
        "- Promotes visually coherent and less noisy predictions.\n",
        "\n",
        "**Vox Morph Loss:**\n",
        "- Combines cross-correlation loss and smoothing loss for voxel-based segmentation tasks.\n",
        "- Encourages accurate segmentation and smooth transitions between neighboring voxels.\n",
        "\n",
        "**Dice Score:**\n",
        "- A common metric for segmentation evaluation.\n",
        "- Compares the overlap and union of predicted and target tensors.\n",
        "- Higher values indicate better segmentation performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqu05iPORXFF"
      },
      "outputs": [],
      "source": [
        "def cross_correlation_loss(I, J, n=9):\n",
        "    I = I.permute(0, 3, 1, 2).to(device)\n",
        "    J = J.permute(0, 3, 1, 2).to(device)\n",
        "    batch_size, channels, xdim, ydim = I.shape\n",
        "    I2 = torch.mul(I, I).to(device)\n",
        "    J2 = torch.mul(J, J).to(device)\n",
        "    IJ = torch.mul(I, J).to(device)\n",
        "    sum_filter = torch.ones((1, channels, n, n)).to(device)\n",
        "\n",
        "    I_sum  = torch.conv2d(I,  sum_filter, padding=1, stride=1)\n",
        "    J_sum  = torch.conv2d(J,  sum_filter, padding=1, stride=1)\n",
        "    I2_sum = torch.conv2d(I2, sum_filter, padding=1, stride=1)\n",
        "    J2_sum = torch.conv2d(J2, sum_filter, padding=1, stride=1)\n",
        "    IJ_sum = torch.conv2d(IJ, sum_filter, padding=1, stride=1)\n",
        "    win_size = n ** 2\n",
        "    u_I = I_sum / win_size\n",
        "    u_J = J_sum / win_size\n",
        "    cross = IJ_sum - u_J * I_sum - u_I * J_sum + u_I * u_J * win_size\n",
        "    I_var = I2_sum - 2 * u_I * I_sum + u_I * u_I * win_size\n",
        "    J_var = J2_sum - 2 * u_J * J_sum + u_J * u_J * win_size\n",
        "    cc = cross * cross / (I_var * J_var + torch.finfo(torch.float32).eps)\n",
        "    return torch.mean(cc)\n",
        "\n",
        "def smoothing_loss(y_pred):\n",
        "    dy = torch.abs(y_pred[:, 1:, :, :] - y_pred[:, :-1, :, :])\n",
        "    dx = torch.abs(y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :])\n",
        "\n",
        "    dx = torch.mul(dx, dx)\n",
        "    dy = torch.mul(dy, dy)\n",
        "    d = torch.mean(dx) + torch.mean(dy)\n",
        "    return d/2.0\n",
        "\n",
        "def dice_score(pred, target):\n",
        "    top = 2 *  torch.sum(pred * target, [1, 2, 3])\n",
        "    union = torch.sum(pred + target, [1, 2, 3])\n",
        "    eps = torch.ones_like(union) * 1e-5\n",
        "    bottom = torch.max(union, eps)\n",
        "    dice = torch.mean(top / bottom)\n",
        "    return dice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMdBdbS_WZRh"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8EceHzaJUh"
      },
      "source": [
        "**Overview of the FIRE Dataset**\n",
        "\n",
        "The Fundus Image Registration (FIRE) Dataset, detailed at [FIRE Dataset](https://projects.ics.forth.gr/cvrl/fire/), is a specialized collection for image registration research in ophthalmology. It comprises 134 high-resolution fundus photographs, each with a resolution of 2912x2912 pixels. These images are divided into 45 retinal pairs, offering a diverse range of pathologies and anatomical features. This dataset provides a robust platform for evaluating the VoxelMorph framework's accuracy in aligning intricate details in retinal images, crucial for advanced medical image analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrWE8AXqagep"
      },
      "outputs": [],
      "source": [
        "!pip install requests py7zr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrgEJUHCah8U"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import py7zr\n",
        "\n",
        "# URL of the file you want to download\n",
        "url = \"https://projects.ics.forth.gr/cvrl/fire/FIRE.7z\"\n",
        "\n",
        "# Destination file name\n",
        "output_file = \"FIRE.7z\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "with open(output_file, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Extract the 7z archive\n",
        "with py7zr.SevenZipFile(output_file, mode='r') as archive:\n",
        "    archive.extractall()\n",
        "\n",
        "DATA_PATH = './FIRE/Images/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFZIjT5cWY04"
      },
      "outputs": [],
      "source": [
        "class my_dataset(Dataset):\n",
        "    def __init__(self, list_IDs):\n",
        "        self.list_IDs = list_IDs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ID = self.list_IDs[index]\n",
        "        fixed_image = torch.Tensor(resize(io.imread(DATA_PATH + ID + '_1.jpg'), (256, 256, 3)))\n",
        "        moving_image = torch.Tensor(resize(io.imread(DATA_PATH + ID + '_2.jpg'), (256, 256, 3)))\n",
        "        return fixed_image, moving_image\n",
        "\n",
        "\n",
        "filename = list(set([x.split('_')[0] for x in os.listdir(DATA_PATH)]))\n",
        "\n",
        "partition = {}\n",
        "partition['train'], partition['validation'] = train_test_split(filename, test_size=0.33, random_state=42)\n",
        "\n",
        "train_dataset = my_dataset(partition['train'])\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "\n",
        "val_dataset = my_dataset(partition['validation'])\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYaC6jDKYYor"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-amLkvBX_-N"
      },
      "source": [
        "### Unet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YSVMLpn01L1"
      },
      "source": [
        "The `UNet` class represents the UNet architecture, a popular neural network model used for semantic segmentation tasks. It consists of contracting and expansive blocks, a bottleneck layer, and decoding layers.\n",
        "\n",
        "- **Contracting Block** consists of two convolutional layers with batch normalization and ReLU activation. This block reduces the spatial dimensions of the input image.\n",
        "\n",
        "- **Expansive Block** consists of convolutional layers, batch normalization, ReLU activation, and transposed convolutional layers. This block increases the spatial dimensions of the input image.\n",
        "\n",
        "- **Final Block** consists of two convolutional layers with batch normalization and ReLU activation. This block is used to obtain the final segmentation output.\n",
        "\n",
        "- **Crop and Concatenate** method crops the output of the contracting block and concatenates it with the corresponding layer from the expansive block. This operation helps preserve spatial information during the decoding process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fYZ6W0VT5Vn"
      },
      "outputs": [],
      "source": [
        "def conv_block(in_channels, out_channels, kernel_size=3, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "def conv_trans_block(in_channels, out_channels, kernel_size=3,  stride=2, padding=1, output_padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(UNet, self).__init__()\n",
        "        # Hint: Use conv_block and conv_trans_block to define enode, bottleneck and decode layers\n",
        "        # Your code here [15 score]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Hint: Implement the forward pass, ensuring to concatenate the skip connections in the decoding path\n",
        "        # Your code here [15 score]\n",
        "\n",
        "        return final_layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIZsjXHjYFE0"
      },
      "source": [
        "### Spatial Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88yRowus0eED"
      },
      "source": [
        "The `SpatialTransformation` class is a module used for image registration and spatial transformation. It provides methods for creating a meshgrid, repeating tensors, and interpolating pixel values.\n",
        "\n",
        "- **Meshgrid**: Generates a 2D grid of coordinates (X and Y values) based on the input height and width.\n",
        "\n",
        "- **Repeat**: Replicates a tensor multiple times along a specified axis.\n",
        "\n",
        "- **Interpolate**: Performs bilinear interpolation to estimate pixel values at non-integer coordinates.\n",
        "\n",
        "- **Forward**: Applies spatial transformation to a moving image using a deformation matrix. Computes new coordinates, performs interpolation, and returns the transformed image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixgppY2DXyy3"
      },
      "outputs": [],
      "source": [
        "class SpatialTransformation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialTransformation, self).__init__()\n",
        "\n",
        "    def meshgrid(self, height, width):\n",
        "        # Hint: Implement the meshgrid function to create a grid of coordinates\n",
        "        # Your code here [10 score]\n",
        "\n",
        "        return x_t, y_t\n",
        "\n",
        "    def repeat(self, x, n_repeats):\n",
        "        # Hint: Implement the repeat function to repeat elements of a tensor\n",
        "        # Your code here [10 score]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def interpolate(self, im, x, y):\n",
        "        # Hint: Implement the interpolate function for bilinear interpolation\n",
        "        # Your code here [20 score]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def forward(self, moving_image, deformation_matrix):\n",
        "        dx = deformation_matrix[:, :, :, 0]\n",
        "        dy = deformation_matrix[:, :, :, 1]\n",
        "\n",
        "        batch_size, height, width = dx.shape\n",
        "\n",
        "        x_mesh, y_mesh = self.meshgrid(height, width)\n",
        "\n",
        "        x_mesh = x_mesh.expand([batch_size, height, width])\n",
        "        y_mesh = y_mesh.expand([batch_size, height, width])\n",
        "\n",
        "        x_new = dx + x_mesh\n",
        "        y_new = dy + y_mesh\n",
        "\n",
        "        return self.interpolate(moving_image, x_new, y_new)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbkxW1_eYIof"
      },
      "source": [
        "### VoxelMorph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i87Nd7x0Rlel"
      },
      "outputs": [],
      "source": [
        "class VoxelMorph(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(VoxelMorph, self).__init__()\n",
        "        self.unet = UNet(in_channels*2, 2)\n",
        "        self.spatial_transform = SpatialTransformation()\n",
        "\n",
        "    def forward(self, moving_image, fixed_image):\n",
        "        x = torch.cat([moving_image, fixed_image], dim=3).permute(0,3,1,2)\n",
        "        deformation_matrix = self.unet(x).permute(0,2,3,1)\n",
        "        registered_image = self.spatial_transform(moving_image, deformation_matrix)\n",
        "        return registered_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmD9-4A_Rtgx"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvOcT2xiUKPO"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "num_channels = train_dataset[0][0].shape[2]\n",
        "voxelmorph = VoxelMorph(num_channels).to(device)\n",
        "optimizer = optim.SGD(voxelmorph.parameters(), lr=1e-4, momentum=0.99)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss, train_dice_score, val_loss, val_dice_score = 0, 0, 0, 0\n",
        "    num_train_batches, num_val_batches = len(train_loader), len(val_loader)\n",
        "\n",
        "    # Training Phase\n",
        "    voxelmorph.train()\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Training')\n",
        "    for batch_fixed, batch_moving in train_loader_tqdm:\n",
        "        batch_fixed, batch_moving = batch_fixed.to(device), batch_moving.to(device)\n",
        "\n",
        "        # Hint: Perform the forward pass and compute the loss\n",
        "        # Your code here [5 score]\n",
        "\n",
        "        # Hint: Perform the backward pass and optimization step\n",
        "        # Your code here [5 score]\n",
        "\n",
        "        # Update progress bar\n",
        "        train_loader_tqdm.set_postfix(train_loss=train_loss / (train_loader_tqdm.n + 1))\n",
        "        train_loader_tqdm.update()\n",
        "\n",
        "    train_loss /= num_train_batches\n",
        "    train_dice_score /= num_train_batches\n",
        "\n",
        "    # Validation Phase\n",
        "    voxelmorph.eval()\n",
        "    val_loader_tqdm = tqdm(val_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Validation')\n",
        "    with torch.no_grad():\n",
        "        for batch_fixed, batch_moving in val_loader_tqdm:\n",
        "            batch_fixed, batch_moving = batch_fixed.to(device), batch_moving.to(device)\n",
        "\n",
        "            # Hint: Perform the forward pass for validation and compute the loss\n",
        "            # Your code here [5 score]\n",
        "\n",
        "            # Update progress bar\n",
        "            val_loader_tqdm.set_postfix(val_loss=val_loss / (val_loader_tqdm.n + 1))\n",
        "            val_loader_tqdm.update()\n",
        "\n",
        "    val_loss /= num_val_batches\n",
        "    val_dice_score /= num_val_batches\n",
        "\n",
        "    # Print epoch summary\n",
        "    epoch_duration = (time.time() - epoch_start_time) / 60\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Duration: {epoch_duration:.2f} mins - Train Loss: {train_loss:.3f}, Train DICE: {train_dice_score:.3f}, Val Loss: {val_loss:.3f}, Val DICE: {val_dice_score:.3f}\")\n",
        "\n",
        "\n",
        "    # Your output has 15 score out of 100. (based on the loss, DICE, and the plot of the next section)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C898zalF4m0"
      },
      "outputs": [],
      "source": [
        "def plot_samples(batch_fixed, batch_moving, registered_image, k):\n",
        "    \"\"\"\n",
        "    Plot k samples from batch_fixed, batch_moving, and registered_image.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(k, 3, figsize=(15, 5 * k))  # Adjust the figure size as needed\n",
        "\n",
        "    # Ensure that batch_fixed, batch_moving, and registered_image are lists of tensors\n",
        "    if not isinstance(batch_fixed, list):\n",
        "        batch_fixed = [batch_fixed]\n",
        "    if not isinstance(batch_moving, list):\n",
        "        batch_moving = [batch_moving]\n",
        "    if not isinstance(registered_image, list):\n",
        "        registered_image = [registered_image]\n",
        "\n",
        "    for i in range(k):\n",
        "        # Plot fixed image\n",
        "        if i < len(batch_fixed):\n",
        "            axs[i, 0].imshow(batch_fixed[i].cpu().squeeze(), cmap='gray')\n",
        "            axs[i, 0].set_title(f'Sample {i+1} - Fixed Image')\n",
        "            axs[i, 0].axis('off')\n",
        "\n",
        "        # Plot moving image\n",
        "        if i < len(batch_moving):\n",
        "            axs[i, 1].imshow(batch_moving[i].cpu().squeeze(), cmap='gray')\n",
        "            axs[i, 1].set_title(f'Sample {i+1} - Moving Image')\n",
        "            axs[i, 1].axis('off')\n",
        "\n",
        "        # Plot registered image\n",
        "        if i < len(registered_image):\n",
        "            axs[i, 2].imshow(registered_image[i].cpu().squeeze(), cmap='gray')\n",
        "            axs[i, 2].set_title(f'Sample {i+1} - Registered Image')\n",
        "            axs[i, 2].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage (within your training or validation loop, or after a forward pass)\n",
        "plot_samples(batch_fixed, batch_moving, registered_image, k=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
